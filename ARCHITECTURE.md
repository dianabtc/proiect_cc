# ğŸ—ï¸ ArhitecturÄƒ Booking Platform - DocumentaÈ›ie TehnicÄƒ CompletÄƒ

## ğŸ“‹ Cuprins

1. [Prezentare GeneralÄƒ](#prezentare-generalÄƒ)
2. [Arhitectura AplicaÈ›iei](#arhitectura-aplicaÈ›iei)
3. [Componente Detaliate](#componente-detaliate)
4. [Kubernetes Dashboard](#kubernetes-dashboard---microserviciu-utilitar-grafic)
5. [Flow-uri de Date](#flow-uri-de-date)
6. [Networking È™i Comunicare](#networking-È™i-comunicare)
7. [Deployment È™i Orchestrare](#deployment-È™i-orchestrare)
8. [Securitate È™i RBAC](#securitate-È™i-rbac)
9. [Deployment Guide](#deployment-guide)
10. [Troubleshooting](#troubleshooting)

---

## Prezentare GeneralÄƒ

**Booking Platform** este o aplicaÈ›ie de rezervare sÄƒli evenimente implementatÄƒ cu arhitecturÄƒ de **microservicii** pe **Kubernetes**. AplicaÈ›ia demonstreazÄƒ concepte cloud-native: containerizare, orchestrare, service discovery, persistent storage È™i monitoring.

### ğŸ¯ Scopul AplicaÈ›iei

- **Utilizatori** pot rezerva sÄƒli pentru evenimente
- **Administratori** gestioneazÄƒ sÄƒlile È™i toate rezervÄƒrile
- **Validare automatÄƒ** a conflictelor de rezervÄƒri
- **Autentificare JWT** cu roluri (USER/ADMIN)
- **Management cluster** prin Kubernetes Dashboard

### ğŸ› ï¸ Stack Tehnologic

| ComponentÄƒ | Tehnologie | Versiune |
|------------|------------|----------|
| Backend Framework | FastAPI | 0.110.0 |
| Database | MySQL | 8.0 |
| ORM | SQLAlchemy | 2.0 |
| Autentificare | JWT (PyJWT) | - |
| Container Runtime | Docker | - |
| Orchestrare | Kubernetes | 1.24+ |
| Package Manager | Helm | 3.x |
| Ingress Controller | NGINX | - |
| Management UI | Kubernetes Dashboard | v2.7.0 |
| DB Admin UI | Adminer | latest |

---

## Arhitectura AplicaÈ›iei

### ğŸ¨ Diagrama Arhitecturii Generale

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Internet / User                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  NGINX Ingress Controller â”‚
                    â”‚   (Kubernetes Ingress)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                        â”‚                        â”‚
   â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€             â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€            â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€
   â”‚  /auth  â”‚             â”‚ /reserv.â”‚            â”‚ /dashboardâ”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
        â”‚                       â”‚                        â”‚
        â”‚                       â”‚                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Auth Service  â”‚â”€â”€â”€â”€â–¶â”‚ Reservation Svc  â”‚    â”‚   Kubernetes      â”‚
â”‚   (FastAPI)    â”‚     â”‚    (FastAPI)     â”‚    â”‚   Dashboard       â”‚
â”‚                â”‚     â”‚                  â”‚    â”‚   (WebUI)         â”‚
â”‚ Port: 8000     â”‚     â”‚ Port: 8000       â”‚    â”‚ Port: 8443        â”‚
â”‚ NS: default    â”‚     â”‚ NS: default      â”‚    â”‚ NS: k8s-dashboard â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                       â”‚                         â”‚
        â”‚                       â”‚                         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
                â”‚                                         â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
        â”‚     MySQL 8      â”‚                              â”‚
        â”‚                  â”‚                              â”‚
        â”‚  - auth_db       â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  - reservation_dbâ”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  Kubernetes API   â”‚
        â”‚                  â”‚   Monitorizare     â”‚  Server           â”‚
        â”‚ Port: 3306       â”‚   Resurse          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚ NS: default      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ PersistentVolume â”‚
        â”‚   (mysql-pvc)    â”‚
        â”‚     1Gi          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Componente Auxiliare                                        â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚  â”‚   Adminer    â”‚  (DB Management UI)                       â”‚
â”‚  â”‚ NodePort     â”‚  Accesibil extern pentru admin DB        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ“¦ Componente Principale

| ComponentÄƒ | Namespace | Replicas | Resurse | Rol |
|------------|-----------|----------|---------|-----|
| auth-service | default | 1 | CPU/Mem: default | Autentificare utilizatori |
| reservation-service | default | 1 | CPU/Mem: default | LogicÄƒ business rezervÄƒri |
| mysql | default | 1 | CPU/Mem: default | PersistenÈ›Äƒ date |
| adminer | default | 1 | CPU/Mem: default | Admin interfaÈ›Äƒ DB |
| kubernetes-dashboard | kubernetes-dashboard | 1 | CPU/Mem: default | Management cluster |
| dashboard-metrics-scraper | kubernetes-dashboard | 1 | CPU/Mem: default | Colectare metrici |

---

## Componente Detaliate

### 1. Auth Service ğŸ”

**ResponsabilitÄƒÈ›i:**
- Ãnregistrare utilizatori noi
- Autentificare cu username/password
- Generare JWT tokens
- Validare tokens pentru alte servicii

**API Endpoints:**

```python
POST /auth/register
{
  "username": "john_doe",
  "password": "secure_pass"
}
Response: {"message": "User registered successfully"}

POST /auth/login
{
  "username": "john_doe",
  "password": "secure_pass"
}
Response: {"access_token": "eyJhbGc..."}

GET /auth/validate
Headers: Authorization: Bearer <token>
Response: {"valid": true, "payload": {"sub": "john_doe", "role": "USER"}}
```

**Model Date:**
```python
class User:
    id: int (PK)
    username: str (unique)
    password: str (hashed with bcrypt)
    role: str (default="USER", options: USER|ADMIN)
```

**Baza de Date:** `auth_db` Ã®n MySQL

**Environment Variables:**
```yaml
DATABASE_URL: mysql+pymysql://root:password@mysql:3306/auth_db
JWT_SECRET_KEY: your-secret-key
JWT_ALGORITHM: HS256
JWT_EXPIRATION_HOURS: 24
```

**Securitate:**
- Passwords sunt hash-uite cu **bcrypt**
- JWT tokens conÈ›in: `sub` (username), `role`, `exp` (expiration)
- Tokens expirÄƒ dupÄƒ 24h

---

### 2. Reservation Service ğŸŸï¸

**ResponsabilitÄƒÈ›i:**
- CRUD pentru sÄƒli evenimente (Event Halls) - doar ADMIN
- Creare rezervÄƒri - USER
- Verificare disponibilitate - PUBLIC
- Anulare rezervÄƒri - USER (proprii) / ADMIN (toate)
- Validare conflicte temporale

**API Endpoints:**

**Halls Management (ADMIN only):**
```python
POST /reservation/halls
Headers: Authorization: Bearer <admin-token>
{
  "name": "Conference Hall A",
  "location": "Building 1, Floor 2",
  "capacity": 50
}

GET /reservation/halls  # PUBLIC
Response: [{"id": 1, "name": "...", "location": "...", "capacity": 50}]

PATCH /reservation/halls/{id}  # ADMIN only
DELETE /reservation/halls/{id}  # ADMIN only
```

**Reservations:**
```python
GET /reservation/availability?hall_id=1&date=2026-01-20&start_time=10:00&end_time=12:00
Response: {"available": true}

POST /reservation/reservations
Headers: Authorization: Bearer <token>
{
  "hall_id": 1,
  "date": "2026-01-20",
  "start_time": "10:00",
  "end_time": "12:00"
}

GET /reservation/reservations
# USER: vede doar ale lui
# ADMIN: vede toate

POST /reservation/reservations/{id}/cancel
# USER: doar ale lui
# ADMIN: orice rezervare
```

**Modele Date:**
```python
class EventHall:
    id: int (PK)
    name: str (unique)
    location: str
    capacity: int

class Reservation:
    id: int (PK)
    user_sub: str (username din JWT)
    hall_id: int (FK â†’ EventHall)
    date: Date
    start_time: Time
    end_time: Time
    status: Enum(ACTIVE, CANCELLED)
```

**LogicÄƒ Business - Validare Conflicte:**

O rezervare este validÄƒ DOAR dacÄƒ **nu existÄƒ altÄƒ rezervare ACTIVÄ‚** care se suprapune:

```python
def has_conflict(hall_id, date, start_time, end_time):
    """
    Conflict existÄƒ dacÄƒ:
    - AceeaÈ™i salÄƒ (hall_id)
    - AceeaÈ™i datÄƒ (date)
    - Status = ACTIVE
    - Overlap temporal: existing.start < new.end AND new.start < existing.end
    """
    return db.query(Reservation).filter(
        Reservation.hall_id == hall_id,
        Reservation.date == date,
        Reservation.status == "ACTIVE",
        Reservation.start_time < end_time,
        start_time < Reservation.end_time
    ).first() is not None
```

**Autorizare:**
- FoloseÈ™te **Auth Service** pentru validare token
- Extrage `role` È™i `sub` din JWT payload
- Enforcement la nivel de endpoint cu decorators

**Baza de Date:** `reservation_db` Ã®n MySQL

---

### 3. MySQL Database ğŸ—„ï¸

**Configurare:**
```yaml
Image: mysql:8
Port: 3306
Root Password: password (âš ï¸ schimbÄƒ Ã®n producÈ›ie!)
Databases:
  - auth_db        # Pentru Auth Service
  - reservation_db # Pentru Reservation Service
```

**PersistenÈ›Äƒ:**
- **PersistentVolumeClaim (PVC)**: `mysql-pvc`
- **Size**: 1Gi
- **StorageClass**: default (depinde de cluster)
- **AccessMode**: ReadWriteOnce

**Init Database:**
- Tabelele sunt create automat de SQLAlchemy prin `Base.metadata.create_all()`
- La primul start, MySQL creeazÄƒ bazele de date
- Schema este gestionatÄƒ de ORM (models.py)

**Backup Strategy (pentru producÈ›ie):**
```bash
# Backup
kubectl exec deployment/mysql -- mysqldump -u root -ppassword auth_db > backup.sql

# Restore
kubectl exec -i deployment/mysql -- mysql -u root -ppassword auth_db < backup.sql
```

---

### 4. Adminer - DB Management UI ğŸ’»

**Configurare:**
```yaml
Image: adminer
Port: 8080
Service Type: NodePort
```

**Acces:**
```bash
kubectl get svc adminer  # vezi NodePort
# AcceseazÄƒ: http://<node-ip>:<nodeport>
```

**CredenÈ›iale:**
- Server: `mysql`
- Username: `root`
- Password: `password`
- Database: `auth_db` sau `reservation_db`

**Use Cases:**
- InspecÈ›ie manualÄƒ baze de date
- Debug schema issues
- Query manual pentru testing
- Vizualizare relaÈ›ii Ã®ntre tabele

---

## Kubernetes Dashboard - Microserviciu Utilitar Grafic

### ğŸ¯ Scop È™i FuncÈ›ionalitate

**Kubernetes Dashboard** este instrumentul oficial de management vizual pentru clustere Kubernetes. Ãn acest proiect, Dashboard-ul serveÈ™te ca **microserviciu utilitar grafic** care oferÄƒ:

âœ… **Vizualizare Ã®n timp real** a tuturor resurselor din cluster
âœ… **Monitorizare** CPU, memorie, status pod-uri
âœ… **Acces la logs** din orice container
âœ… **Management** deployments, scaling, restart
âœ… **Debugging** prin events È™i resource inspection

### ğŸ“¦ Arhitectura Dashboard-ului

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Namespace: kubernetes-dashboard                    â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ kubernetes-dashboard   â”‚    â”‚ metrics-scraper         â”‚ â”‚
â”‚  â”‚ Pod                    â”‚â”€â”€â”€â–¶â”‚ Pod                     â”‚ â”‚
â”‚  â”‚                        â”‚    â”‚                         â”‚ â”‚
â”‚  â”‚ - InterfaÈ›Äƒ WebUI      â”‚    â”‚ - Colectare metrici    â”‚ â”‚
â”‚  â”‚ - HTTPS (8443)         â”‚    â”‚ - CPU/Memory graphs    â”‚ â”‚
â”‚  â”‚ - Token auth           â”‚    â”‚ - HTTP (8000)          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚           â”‚                                                  â”‚
â”‚           â”‚ ServiceAccount: kubernetes-dashboard            â”‚
â”‚           â”‚ ClusterRoleBinding â†’ ClusterRole                â”‚
â”‚           â”‚                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Kubernetes API    â”‚  â† Dashboard citeÈ™te TOATE resursele
    â”‚ Server            â”‚    din cluster prin API
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â”œâ”€â–º Namespace: default (aplicaÈ›ia ta)
            â”‚   â”œâ”€ auth-service pods, logs, metrics
            â”‚   â”œâ”€ reservation-service pods, logs, metrics
            â”‚   â”œâ”€ mysql pods, PVC, logs
            â”‚   â”œâ”€ Services, Ingress, ConfigMaps
            â”‚   â””â”€ Events pentru debugging
            â”‚
            â””â”€â–º Namespace: kubernetes-dashboard (el Ã®nsuÈ™i)
                â””â”€ Propriile resurse
```

### ğŸ” RBAC È™i Securitate

**Dashboard-ul foloseÈ™te 3 nivele de permisiuni:**

1. **ServiceAccount: kubernetes-dashboard**
   - Acces la propriile resurse (secrets, configmaps)
   - Proxy cÄƒtre metrics scraper

2. **ClusterRole: kubernetes-dashboard-readonly**
   ```yaml
   # Permissions read-only pentru:
   - pods, services, nodes, namespaces
   - deployments, replicasets, statefulsets
   - ingresses, configmaps, secrets (metadata only)
   - persistentvolumes, persistentvolumeclaims
   - events
   ```

3. **ServiceAccount: admin-user** (pentru autentificare)
   - ClusterRoleBinding cÄƒtre **cluster-admin**
   - Acces complet la toate resursele
   - Folosit pentru generare token JWT

**Flow Autentificare:**
```
1. User genereazÄƒ token:
   $ kubectl -n kubernetes-dashboard create token admin-user
   
2. Token este JWT cu:
   - Subject: admin-user
   - Permissions: cluster-admin (toate)
   - Expiration: 1h (default)

3. Dashboard verificÄƒ token prin Kubernetes API
   - Token valid â†’ acces la toate resursele
   - Token invalid/expirat â†’ error 401
```

### ğŸŒ Networking È™i Acces

**Service Configuration:**
```yaml
apiVersion: v1
kind: Service
metadata:
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard
spec:
  type: ClusterIP
  ports:
    - port: 8443
      targetPort: 8443
      protocol: TCP
  selector:
    app: kubernetes-dashboard
```

**Ingress Configuration:**
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: dashboard-ingress
  namespace: kubernetes-dashboard
  annotations:
    nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
    nginx.ingress.kubernetes.io/rewrite-target: /$2
spec:
  ingressClassName: nginx
  rules:
    - http:
        paths:
          - path: /dashboard(/|$)(.*)
            pathType: ImplementationSpecific
            backend:
              service:
                name: kubernetes-dashboard
                port:
                  number: 8443
```

**Metode de Acces:**

| MetodÄƒ | ComandÄƒ | URL | Use Case |
|--------|---------|-----|----------|
| Port Forward | `kubectl port-forward -n kubernetes-dashboard svc/kubernetes-dashboard 8443:8443` | https://localhost:8443 | Development local |
| NodePort | ModificÄƒ service type Ã®n values.yaml | https://\<node-ip\>:30443 | Testing, lab environment |
| Ingress | Deploy cu Ingress configurat | https://\<domain\>/dashboard | Production |

### ğŸ“Š Ce Poate Monitoriza Dashboard-ul?

**Ãn namespace `default` (aplicaÈ›ia ta):**

| Resurse | Ce Vezi | AcÈ›iuni Posibile |
|---------|---------|------------------|
| **Deployments** | auth-service, reservation-service, mysql, adminer | Scale replicas, restart, edit |
| **Pods** | Status (Running/Error), restart count, age | View logs, exec shell, delete |
| **Services** | ClusterIP, ports, endpoints | Edit, view endpoints |
| **Ingress** | Paths (/auth, /reservation), backends | View config, rules |
| **PVC** | mysql-pvc status, size, binding | View details, check storage |
| **ConfigMaps** | Configuration data | Edit values |
| **Secrets** | ExistÄƒ, dar values sunt hidden | View metadata |
| **Events** | Deployment events, warnings, errors | Filter, sort, debug |

**Metrics Disponibile:**
- CPU usage per pod (grafic Ã®n timp)
- Memory usage per pod (grafic Ã®n timp)
- Network I/O (dacÄƒ metrics-server instalat)
- Disk usage pentru volumes

**Real-time Logs:**
```
Dashboard â†’ Pods â†’ click pe pod â†’ Logs icon
- Follow logs Ã®n timp real
- Download logs
- Filter by timestamp
- Previous container logs (dacÄƒ a crashuit)
```

### ğŸ” Integration cu AplicaÈ›ia

**Dashboard-ul NU modificÄƒ aplicaÈ›ia, ci doar o OBSERVÄ‚:**

âŒ **Dashboard NU:**
- Nu intercepteazÄƒ request-uri HTTP
- Nu modificÄƒ codul aplicaÈ›iei
- Nu acceseazÄƒ direct MySQL
- Nu afecteazÄƒ performance-ul

âœ… **Dashboard POATE:**
- Vedea toate pod-urile È™i status-ul lor
- Accesa logs pentru debugging
- Monitoriza resource usage
- Scale deployments (change replicas)
- Restart pods (delete â†’ K8s recreeazÄƒ)
- Vizualiza networking (services, ingress)
- Debug prin events

**Exemplu Flow Debugging:**
```
Scenario: Auth Service nu rÄƒspunde

1. Dashboard â†’ Switch la namespace "default"
2. Workloads â†’ Pods â†’ auth-service-xxx
3. Status: CrashLoopBackOff (RED)
4. Click pe pod â†’ Logs
5. Vezi error: "Cannot connect to MySQL at mysql:3306"
6. Navigate to Services â†’ mysql
7. Status: Running, Endpoints: OK
8. Navigate to Events
9. Vezi: "Warning BackOff pod/auth-service-xxx"
10. Fix: Verifici DATABASE_URL Ã®n ConfigMap
11. DupÄƒ fix, Dashboard aratÄƒ pod revine la Running (GREEN)
```

### ğŸš€ Deployment È™i Configurare

**1. Helm Values (`values.yaml`):**
```yaml
dashboard:
  replicaCount: 1
  image:
    repository: kubernetesui/dashboard
    tag: v2.7.0
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP      # Sau NodePort pentru acces extern direct
    port: 8443
    nodePort: 30443      # Doar dacÄƒ type: NodePort
```

**2. Deploy cu Helm:**
```bash
helm upgrade --install booking-platform . --namespace default
```

Helm va crea automat:
- Namespace `kubernetes-dashboard`
- Toate resursele Dashboard-ului
- RBAC (ServiceAccounts, ClusterRoles, Bindings)
- Ingress pentru acces

**3. Verificare Deployment:**
```bash
# Check pods
kubectl get pods -n kubernetes-dashboard

# Ar trebui sÄƒ vezi:
# kubernetes-dashboard-xxx          1/1   Running
# dashboard-metrics-scraper-xxx     1/1   Running

# Check RBAC
kubectl get sa,clusterrole,clusterrolebinding | grep dashboard
```

**4. Acces È™i Autentificare:**
```bash
# GenereazÄƒ token
TOKEN=$(kubectl -n kubernetes-dashboard create token admin-user)
echo $TOKEN

# Port forward
kubectl port-forward -n kubernetes-dashboard svc/kubernetes-dashboard 8443:8443

# Deschide browser
open https://localhost:8443

# Login:
# 1. SelecteazÄƒ "Token"
# 2. Paste $TOKEN
# 3. Click "Sign In"
```

### ğŸ“ˆ Use Cases pentru Proiectul TÄƒu

**1. DemonstraÈ›ie ArhitecturÄƒ Microservicii:**
```
"AplicaÈ›ia noastrÄƒ are 4 microservicii independente..."
â†’ AratÄƒ Ã®n Dashboard: Workloads â†’ Deployments
â†’ Vizualizare clarÄƒ: auth, reservation, mysql, adminer
```

**2. Health Monitoring:**
```
"Toate serviciile sunt healthy È™i running..."
â†’ Dashboard: Pods â†’ Verde checks pentru toate
â†’ CPU/Memory Ã®n limite normale
```

**3. Scaling Demonstration:**
```
"Putem scala orizontal foarte uÈ™or..."
â†’ Dashboard: Deployments â†’ auth-service â†’ Edit
â†’ Change replicas: 1 â†’ 3
â†’ Vezi instant 3 pods auth-service
```

**4. Debugging Real-time:**
```
"DacÄƒ avem o problemÄƒ, o putem debug instant..."
â†’ Dashboard: Pods â†’ reservation-service â†’ Logs
â†’ Follow logs, vezi requests incoming
```

**5. Persistent Storage:**
```
"Datele sunt persistente prin PersistentVolumeClaim..."
â†’ Dashboard: Config and Storage â†’ PVCs
â†’ mysql-pvc: Bound, 1Gi
```

**6. Resource Management:**
```
"MonitorizÄƒm continuu utilizarea resurselor..."
â†’ Dashboard: Grafice CPU/Memory per pod
â†’ MySQL consumÄƒ cel mai mult (evident)
```

---

## Flow-uri de Date

### ğŸ”„ Flow 1: Ãnregistrare Utilizator

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Client  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚ POST /auth/register
     â”‚ {"username": "john", "password": "pass123"}
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ingress NGINX   â”‚
â”‚  /auth â†’ auth:   â”‚
â”‚         8000     â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Auth Service    â”‚
â”‚  1. Verify user  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     not exists   â”‚          â”‚
â”‚  2. Hash passwordâ”‚          â”‚ Query: SELECT * FROM users
â”‚     (bcrypt)     â”‚          â”‚        WHERE username='john'
â”‚  3. Save to DB   â”‚          â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
     â”‚                        â–¼
     â”‚ INSERT INTO users   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ (username, pass,    â”‚   MySQL     â”‚
     â”‚  role='USER')       â”‚   auth_db   â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   Table:    â”‚
                           â”‚   - users   â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚ Response:
     â–¼ {"message": "User registered"}
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Client  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”„ Flow 2: Login È™i ObÈ›inere Token JWT

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Client  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚ POST /auth/login
     â”‚ {"username": "john", "password": "pass123"}
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Auth Service    â”‚
â”‚  1. Get user     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     from DB      â”‚          â”‚ Query: SELECT * FROM users
â”‚  2. Verify       â”‚          â”‚        WHERE username='john'
â”‚     password     â”‚          â”‚
â”‚     (bcrypt)     â”‚          â–¼
â”‚  3. Generate JWT â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  â”‚â—€â”€â”€â”€â”€â”€â”‚   MySQL     â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚   auth_db   â”‚
     â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚ JWT Payload:
     â”‚ {
     â”‚   "sub": "john",
     â”‚   "role": "USER",
     â”‚   "exp": timestamp + 24h
     â”‚ }
     â”‚ Signed with: JWT_SECRET_KEY
     â”‚
     â”‚ Response:
     â”‚ {"access_token": "eyJhbGciOiJIUzI1NiIs..."}
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Client  â”‚ Saves token for future requests
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”„ Flow 3: Creare Rezervare (cu Autentificare È™i Validare)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Client  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚ POST /reservation/reservations
     â”‚ Headers: Authorization: Bearer <JWT_TOKEN>
     â”‚ Body: {"hall_id": 1, "date": "2026-01-20",
     â”‚        "start_time": "10:00", "end_time": "12:00"}
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ingress NGINX       â”‚
â”‚  /reservation â†’      â”‚
â”‚  reservation:8000    â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Reservation Service  â”‚
â”‚ 1. Extract JWT token â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â”‚ GET /auth/validate
     â”‚ Headers: Bearer <token>
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Auth Service        â”‚
â”‚  1. Decode JWT       â”‚â”€â”€â”€â”
â”‚  2. Verify signature â”‚   â”‚ JWT validation:
â”‚  3. Check expiration â”‚   â”‚ - Signature valid?
â”‚  4. Return payload   â”‚â—€â”€â”€â”˜ - Not expired?
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â”‚ Response:
     â”‚ {"valid": true,
     â”‚  "payload": {"sub": "john", "role": "USER"}}
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Reservation Service  â”‚
â”‚ 2. Validate time     â”‚
â”‚    interval          â”‚
â”‚ 3. Check hall exists â”‚â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Check conflicts   â”‚      â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
     â”‚                        â”‚ Queries:
     â”‚                        â”‚ 1. SELECT * FROM event_halls
     â”‚                        â”‚    WHERE id=1
     â”‚                        â”‚
     â”‚                        â”‚ 2. SELECT * FROM reservations
     â”‚                        â”‚    WHERE hall_id=1
     â”‚                        â”‚      AND date='2026-01-20'
     â”‚                        â”‚      AND status='ACTIVE'
     â”‚                        â”‚      AND start_time < '12:00'
     â”‚                        â”‚      AND '10:00' < end_time
     â”‚                        â–¼
     â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚                    â”‚    MySQL        â”‚
     â”‚                    â”‚ reservation_db  â”‚
     â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ Tables:         â”‚
     â”‚  No conflicts      â”‚ - event_halls   â”‚
     â”‚                    â”‚ - reservations  â”‚
     â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â”‚ 5. Create reservation
     â”‚    INSERT INTO reservations
     â”‚    (user_sub='john', hall_id=1,
     â”‚     date, start_time, end_time,
     â”‚     status='ACTIVE')
     â”‚
     â”‚ Response:
     â”‚ {"id": 42, "user_sub": "john",
     â”‚  "hall_id": 1, "date": "2026-01-20",
     â”‚  "start_time": "10:00", "end_time": "12:00",
     â”‚  "status": "ACTIVE"}
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Client  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”„ Flow 4: Listare RezervÄƒri (cu Autorizare pe Rol)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Client  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚ GET /reservation/reservations
     â”‚ Headers: Authorization: Bearer <JWT_TOKEN>
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Reservation Service  â”‚
â”‚ 1. Validate token    â”‚â”€â”€â–¶ Auth Service (validate)
â”‚ 2. Get payload       â”‚â—€â”€â”€ {"sub": "john", "role": "USER"}
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â”‚ role == "USER"?
     â”‚   YES â†’ Query: WHERE user_sub='john'
     â”‚   NO (ADMIN) â†’ Query: (toate)
     â”‚
     â”‚ Query:
     â”‚ SELECT * FROM reservations
     â”‚ WHERE user_sub='john'  -- doar dacÄƒ USER
     â”‚ ORDER BY id DESC
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    MySQL        â”‚
â”‚ reservation_db  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚ Result:
     â”‚ [
     â”‚   {id: 42, user_sub: "john", hall_id: 1, ...},
     â”‚   {id: 38, user_sub: "john", hall_id: 2, ...}
     â”‚ ]
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Reservation Service  â”‚
â”‚ Return filtered list â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Client  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”„ Flow 5: Dashboard Monitorizare

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User Browser    â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚ 1. AcceseazÄƒ: https://localhost:8443
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Kubernetes Dashboard    â”‚
â”‚  Pod (kubernetes-        â”‚
â”‚       dashboard ns)      â”‚
â”‚  1. Cere autentificare   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â”‚ 2. User introduce Token:
     â”‚    kubectl -n kubernetes-dashboard create token admin-user
     â”‚    â†’ "eyJhbGciOiJSUzI1NiIs..."
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Dashboard verificÄƒ      â”‚
â”‚  token prin K8s API      â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Kubernetes API Server   â”‚
â”‚  1. Validate token       â”‚
â”‚  2. Check permissions:   â”‚
â”‚     - admin-user has     â”‚
â”‚       cluster-admin      â”‚
â”‚  3. Allow access         â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â”‚ Token valid + cluster-admin permissions
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Dashboard UI loaded     â”‚
â”‚  User selecteazÄƒ         â”‚
â”‚  namespace: "default"    â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â”‚ 3. Dashboard face API calls pentru resurse:
     â”‚
     â”œâ”€â”€â–¶ GET /api/v1/namespaces/default/pods
     â”‚    Response: [auth-service-xxx, reservation-service-xxx, ...]
     â”‚
     â”œâ”€â”€â–¶ GET /apis/apps/v1/namespaces/default/deployments
     â”‚    Response: [auth-service, reservation-service, mysql, adminer]
     â”‚
     â”œâ”€â”€â–¶ GET /api/v1/namespaces/default/services
     â”‚    Response: [auth-service:8000, reservation-service:8000, ...]
     â”‚
     â””â”€â”€â–¶ GET /api/v1/namespaces/default/persistentvolumeclaims
          Response: [mysql-pvc: Bound, 1Gi]
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Dashboard afiÈ™eazÄƒ:     â”‚
â”‚  âœ“ 4 Deployments         â”‚
â”‚  âœ“ 4 Pods (Running)      â”‚
â”‚  âœ“ 4 Services            â”‚
â”‚  âœ“ 1 PVC (Bound)         â”‚
â”‚  âœ“ CPU/Memory graphs     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

User click pe pod "auth-service-xxx"
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Dashboard face:         â”‚
â”‚  GET /api/v1/namespaces/ â”‚
â”‚      default/pods/       â”‚
â”‚      auth-service-xxx/   â”‚
â”‚      log?follow=true     â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼ Real-time logs stream
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INFO:     Started server â”‚
â”‚ INFO:     Waiting for... â”‚
â”‚ INFO:     POST /auth/... â”‚
â”‚ ...                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Networking È™i Comunicare

### ğŸŒ Kubernetes Services

**Service Discovery:**
- Toate serviciile folosesc **ClusterIP** (internal)
- Kubernetes DNS rezolvÄƒ automat: `service-name.namespace.svc.cluster.local`
- Simplified: `service-name` (Ã®n acelaÈ™i namespace)

**Service Map:**

```yaml
# Auth Service
auth-service.default.svc.cluster.local:8000
  â†’ Selector: app=auth-service
  â†’ Target: auth-service pods

# Reservation Service  
reservation-service.default.svc.cluster.local:8000
  â†’ Selector: app=reservation-service
  â†’ Target: reservation-service pods

# MySQL
mysql.default.svc.cluster.local:3306
  â†’ Selector: app=mysql
  â†’ Target: mysql pod

# Dashboard
kubernetes-dashboard.kubernetes-dashboard.svc.cluster.local:8443
  â†’ Selector: app=kubernetes-dashboard
  â†’ Target: dashboard pod
```

### ğŸšª Ingress Routing

**NGINX Ingress Controller** gestioneazÄƒ external access:

```
External Request: http://your-domain.com/auth/login
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  NGINX Ingress Controller           â”‚
â”‚                                     â”‚
â”‚  Rules:                             â”‚
â”‚  - host: * (all)                    â”‚
â”‚    paths:                           â”‚
â”‚      /auth â†’ auth-service:8000      â”‚
â”‚      /reservation â†’ reservation:    â”‚
â”‚                     8000            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚                    â”‚
  â–¼                    â–¼
auth-service      reservation-service
(ClusterIP)       (ClusterIP)

---

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Dashboard Ingress                  â”‚
â”‚  (namespace: kubernetes-dashboard)  â”‚
â”‚                                     â”‚
â”‚  Rules:                             â”‚
â”‚  - path: /dashboard                 â”‚
â”‚    backend: kubernetes-dashboard:   â”‚
â”‚             8443                    â”‚
â”‚  - annotations:                     â”‚
â”‚      backend-protocol: HTTPS        â”‚
â”‚      rewrite-target: /$2            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    kubernetes-dashboard
    (ClusterIP, HTTPS)
```

**Path Rewriting Example:**
```
Request: http://your-domain.com/dashboard/
  â”‚
  â–¼ Ingress rewrite-target: /$2
  â”‚
  â–¼
https://kubernetes-dashboard:8443/
```

### ğŸ”— Inter-Service Communication

**Reservation Service â†’ Auth Service:**

```python
# reservation-service/app/auth_client.py

AUTH_SERVICE_URL = os.getenv("AUTH_SERVICE_URL", "http://auth-service:8000/auth")

def validate_token(token: str):
    response = requests.get(
        f"{AUTH_SERVICE_URL}/validate",
        headers={"Authorization": f"Bearer {token}"}
    )
    return response.json()
```

**Service-to-Service Flow:**
```
reservation-service pod
  â”‚
  â”‚ HTTP GET http://auth-service:8000/auth/validate
  â”‚
  â–¼
Kubernetes DNS
  â”‚ Resolves: auth-service â†’ ClusterIP (e.g., 10.96.45.123)
  â”‚
  â–¼
auth-service ClusterIP
  â”‚ Load balances to one of auth-service pods
  â”‚
  â–¼
auth-service pod
  â”‚ Processes request
  â”‚ Returns: {"valid": true, "payload": {...}}
  â”‚
  â–¼
reservation-service pod
  â”‚ Receives response
  â”‚ Continues processing
```

### ğŸ“¡ Network Policies (Optional - Production)

Pentru producÈ›ie, poÈ›i restricÈ›iona trafic:

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: reservation-service-netpol
spec:
  podSelector:
    matchLabels:
      app: reservation-service
  policyTypes:
    - Ingress
    - Egress
  ingress:
    - from:
        - podSelector:
            matchLabels:
              app: ingress-nginx  # Doar de la Ingress
      ports:
        - protocol: TCP
          port: 8000
  egress:
    - to:
        - podSelector:
            matchLabels:
              app: auth-service  # Poate comunica cu Auth
        - podSelector:
            matchLabels:
              app: mysql  # Poate comunica cu MySQL
      ports:
        - protocol: TCP
          port: 8000  # auth-service
        - protocol: TCP
          port: 3306  # mysql
```

---

## Deployment È™i Orchestrare

### ğŸ“¦ Helm Chart Structure

```
helm/booking-platform/
â”œâ”€â”€ Chart.yaml              # Metadata chart
â”œâ”€â”€ values.yaml            # ConfigurÄƒri centrale
â”œâ”€â”€ deploy.sh              # Script automat deployment
â”œâ”€â”€ verify-integration.sh  # Script verificare
â””â”€â”€ templates/
    â”œâ”€â”€ auth/
    â”‚   â”œâ”€â”€ deployment.yaml
    â”‚   â””â”€â”€ service.yaml
    â”œâ”€â”€ reservation/
    â”‚   â”œâ”€â”€ deployment.yaml
    â”‚   â””â”€â”€ service.yaml
    â”œâ”€â”€ mysql/
    â”‚   â”œâ”€â”€ deployment.yaml
    â”‚   â”œâ”€â”€ service.yaml
    â”‚   â””â”€â”€ pvc.yaml
    â”œâ”€â”€ adminer/
    â”‚   â”œâ”€â”€ deployment.yaml
    â”‚   â””â”€â”€ service.yaml
    â”œâ”€â”€ dashboard/
    â”‚   â”œâ”€â”€ namespace.yaml
    â”‚   â”œâ”€â”€ deployment.yaml
    â”‚   â”œâ”€â”€ service.yaml
    â”‚   â”œâ”€â”€ serviceaccount.yaml
    â”‚   â”œâ”€â”€ secret.yaml
    â”‚   â”œâ”€â”€ clusterrole.yaml
    â”‚   â”œâ”€â”€ clusterrolebinding.yaml
    â”‚   â”œâ”€â”€ admin-user.yaml
    â”‚   â”œâ”€â”€ metrics-scraper-deployment.yaml
    â”‚   â””â”€â”€ metrics-scraper-service.yaml
    â””â”€â”€ ingress.yaml        # 2 Ingress resources
```

### ğŸš€ Deployment Process

**Metoda 1: Script Automat (Recomandat)**

```bash
cd helm/booking-platform
./deploy.sh
```

Script-ul executÄƒ:
1. âœ… VerificÄƒ Helm instalat
2. âœ… OferÄƒ rebuild Docker images (opÈ›ional)
3. âœ… `helm upgrade --install booking-platform .`
4. âœ… AÈ™teaptÄƒ pods ready (timeout 5min)
5. âœ… GenereazÄƒ Dashboard token
6. âœ… SalveazÄƒ token Ã®n `dashboard-token.txt`
7. âœ… AfiÈ™eazÄƒ instrucÈ›iuni acces
8. âœ… OferÄƒ start port-forward automat

**Metoda 2: Manual cu Helm**

```bash
# 1. Build Docker images
cd auth-service
docker build -t auth-service:latest .

cd ../reservation-service
docker build -t reservation-service:latest .

# 2. Deploy
cd ../helm/booking-platform
helm upgrade --install booking-platform . \
  --namespace default \
  --create-namespace \
  --wait \
  --timeout 5m

# 3. Verificare
kubectl get pods
kubectl get svc
kubectl get ingress -A

# 4. Dashboard token
kubectl -n kubernetes-dashboard create token admin-user
```

### ğŸ”„ Update È™i Rollback

**Update configuraÈ›ie:**

```bash
# ModificÄƒ values.yaml (ex: scale replicas)
vim values.yaml

# Apply changes
helm upgrade booking-platform .

# Sau override values din CLI
helm upgrade booking-platform . --set auth.replicaCount=3
```

**Rollback la versiune anterioarÄƒ:**

```bash
# Vezi history
helm history booking-platform

# Rollback
helm rollback booking-platform 1  # rollback la revision 1
```

### ğŸ§¹ Cleanup

```bash
# Uninstall aplicaÈ›ie
helm uninstall booking-platform

# È˜terge namespace Dashboard
kubectl delete namespace kubernetes-dashboard

# È˜terge PVC (dacÄƒ vrei sÄƒ È™tergi È™i datele)
kubectl delete pvc mysql-pvc -n default
```

---

## Securitate È™i RBAC

### ğŸ” Securitate la Nivel de AplicaÈ›ie

**1. Auth Service:**
- Passwords hash-uite cu **bcrypt** (cost factor: 12)
- JWT tokens signed cu **HS256** (HMAC-SHA256)
- Secret key stocat Ã®n environment variable
- Tokens expirÄƒ dupÄƒ 24h

**2. Reservation Service:**
- Toate endpoint-urile protejate necesitÄƒ JWT valid
- Authorization checks bazate pe `role` din JWT
- USER poate vedea doar rezervÄƒrile proprii
- ADMIN are acces la toate

**3. MySQL:**
- âš ï¸ **Pentru producÈ›ie**: schimbÄƒ `root` password
- CreeazÄƒ utilizatori separaÈ›i pentru fiecare service
- Grant only needed permissions

### ğŸ›¡ï¸ RBAC pentru Dashboard

**ServiceAccounts:**

```yaml
# 1. kubernetes-dashboard (pentru Dashboard Ã®nsuÈ™i)
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard

# 2. admin-user (pentru autentificare users)
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kubernetes-dashboard
```

**ClusterRoles:**

```yaml
# 1. kubernetes-dashboard - basic permissions
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: kubernetes-dashboard
rules:
  - apiGroups: [""]
    resources: ["secrets", "configmaps"]
    verbs: ["get", "update", "delete"]

# 2. kubernetes-dashboard-readonly - view all resources
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: kubernetes-dashboard-readonly
rules:
  - apiGroups: ["", "apps", "batch", "networking.k8s.io"]
    resources: ["*"]
    verbs: ["get", "list", "watch"]
```

**ClusterRoleBindings:**

```yaml
# admin-user â†’ cluster-admin (full access)
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin  # Built-in role: god mode
subjects:
  - kind: ServiceAccount
    name: admin-user
    namespace: kubernetes-dashboard
```

### ğŸ”’ Best Practices Production

1. **Secrets Management:**
   ```bash
   # Nu hardcoda secrets Ã®n values.yaml
   # FoloseÈ™te Kubernetes Secrets
   kubectl create secret generic jwt-secret \
     --from-literal=key=$(openssl rand -base64 32)
   
   kubectl create secret generic mysql-credentials \
     --from-literal=root-password=$(openssl rand -base64 20)
   ```

2. **TLS pentru Ingress:**
   ```yaml
   spec:
     tls:
       - hosts:
           - your-domain.com
         secretName: tls-secret
   ```

3. **Network Policies:**
   - RestricÈ›ioneazÄƒ inter-pod communication
   - Allow only necessary traffic

4. **Pod Security:**
   ```yaml
   securityContext:
     runAsNonRoot: true
     runAsUser: 1000
     fsGroup: 2000
     capabilities:
       drop:
         - ALL
   ```

5. **Resource Limits:**
   ```yaml
   resources:
     requests:
       memory: "256Mi"
       cpu: "250m"
     limits:
       memory: "512Mi"
       cpu: "500m"
   ```

---

## Deployment Guide

### ğŸ“‹ Prerequisites

```bash
# VerificÄƒ instalÄƒri
docker --version          # Docker 20+
kubectl version --client  # Kubernetes 1.24+
helm version             # Helm 3.x
minikube version         # Sau alt cluster (kind, k3s, cloud)

# Start cluster local (dacÄƒ foloseÈ™ti minikube)
minikube start --memory=4096 --cpus=2

# Enable Ingress addon
minikube addons enable ingress
```

### ğŸš€ Deployment Steps

**Pas 1: CloneazÄƒ/NavigheazÄƒ la Proiect**

```bash
cd /path/to/proiect_cc
```

**Pas 2: Build Docker Images**

```bash
# Auth Service
cd auth-service
docker build -t auth-service:latest .

# Reservation Service
cd ../reservation-service
docker build -t reservation-service:latest .

# DacÄƒ foloseÈ™ti minikube, load images Ã®n cluster
eval $(minikube docker-env)
# Re-run build commands
```

**Pas 3: Deploy cu Helm**

```bash
cd ../helm/booking-platform

# Quick deploy cu script
./deploy.sh

# SAU manual
helm upgrade --install booking-platform . \
  --namespace default \
  --create-namespace \
  --wait
```

**Pas 4: Verificare**

```bash
# Check pods
kubectl get pods
kubectl get pods -n kubernetes-dashboard

# Check services
kubectl get svc
kubectl get svc -n kubernetes-dashboard

# Check ingress
kubectl get ingress -A

# Run verification script
./verify-integration.sh
```

**Pas 5: Acces Dashboard**

```bash
# Get token
kubectl -n kubernetes-dashboard create token admin-user

# Port forward
kubectl port-forward -n kubernetes-dashboard \
  service/kubernetes-dashboard 8443:8443

# Browser: https://localhost:8443
# Login cu token
```

**Pas 6: Test API**

```bash
# Register user
curl -X POST http://localhost/auth/register \
  -H "Content-Type: application/json" \
  -d '{"username": "testuser", "password": "test123"}'

# Login
TOKEN=$(curl -s -X POST http://localhost/auth/login \
  -H "Content-Type: application/json" \
  -d '{"username": "testuser", "password": "test123"}' \
  | jq -r '.access_token')

echo "Token: $TOKEN"

# List halls
curl http://localhost/reservation/halls

# Create reservation
curl -X POST http://localhost/reservation/reservations \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "hall_id": 1,
    "date": "2026-01-25",
    "start_time": "14:00",
    "end_time": "16:00"
  }'
```

### ğŸ” Monitoring Ã®n Dashboard

DupÄƒ deployment:

1. **Login Ã®n Dashboard** (https://localhost:8443)
2. **Switch la namespace "default"**
3. **Navigate:**
   - Workloads â†’ Deployments (vezi cele 4)
   - Workloads â†’ Pods (status Running)
   - Services â†’ Services (vezi ports)
   - Config â†’ PersistentVolumeClaims (mysql-pvc)
4. **Test logs real-time:**
   - Click pe auth-service pod
   - Click "Logs" icon
   - Vezi FastAPI logs
5. **Test scaling:**
   - Deployments â†’ auth-service â†’ Edit
   - Change replicas: 1 â†’ 3
   - Vezi 3 pods apar instant

---

## Troubleshooting

### ğŸ› Probleme Comune

#### 1. Pod Ã®n status ImagePullBackOff

**CauzÄƒ:** Docker image nu existÄƒ sau nu e accesibil

**SoluÈ›ie:**
```bash
# VerificÄƒ image-ul
kubectl describe pod <pod-name>

# DacÄƒ foloseÈ™ti minikube, load images
eval $(minikube docker-env)
docker build -t auth-service:latest auth-service/
docker build -t reservation-service:latest reservation-service/

# Sau schimbÄƒ imagePullPolicy
# Ãn values.yaml: pullPolicy: Never (pentru local)
```

#### 2. Pod Ã®n status CrashLoopBackOff

**CauzÄƒ:** Container crashuieÈ™te la start

**SoluÈ›ie:**
```bash
# Vezi logs
kubectl logs <pod-name>
kubectl logs <pod-name> --previous  # logs din crashul anterior

# Common causes:
# - Database connection failed
# - Missing environment variables
# - Port already in use

# Check environment
kubectl exec -it <pod-name> -- env | grep DATABASE
```

#### 3. Service nu rÄƒspunde

**CauzÄƒ:** Service nu gÄƒseÈ™te pods sau pods nu sunt ready

**SoluÈ›ie:**
```bash
# Check endpoints
kubectl get endpoints <service-name>
# DacÄƒ nu sunt endpoints â†’ selector greÈ™it

# VerificÄƒ selector vs labels
kubectl describe svc <service-name>
kubectl get pods --show-labels

# Test direct Ã®n pod
kubectl run test-pod --rm -i --tty --image=curlimages/curl -- sh
curl http://auth-service:8000/health
```

#### 4. MySQL Connection Failed

**CauzÄƒ:** MySQL nu e ready sau credentials greÈ™ite

**SoluÈ›ie:**
```bash
# Check MySQL pod
kubectl logs deployment/mysql

# Test connection
kubectl exec -it deployment/mysql -- mysql -u root -ppassword -e "SHOW DATABASES;"

# VerificÄƒ environment Ã®n service pods
kubectl exec -it deployment/auth-service -- env | grep DATABASE_URL

# Should be: mysql+pymysql://root:password@mysql:3306/auth_db
```

#### 5. Ingress nu funcÈ›ioneazÄƒ

**CauzÄƒ:** Ingress Controller nu e instalat

**SoluÈ›ie:**
```bash
# Check Ingress Controller
kubectl get pods -n ingress-nginx

# DacÄƒ lipseÈ™te, instaleazÄƒ:
# Minikube:
minikube addons enable ingress

# Kind:
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml

# VerificÄƒ Ingress
kubectl describe ingress booking-ingress
kubectl describe ingress dashboard-ingress -n kubernetes-dashboard
```

#### 6. Dashboard Token expirat

**CauzÄƒ:** Tokens JWT expirÄƒ dupÄƒ 1h (default)

**SoluÈ›ie:**
```bash
# GenereazÄƒ nou token
kubectl -n kubernetes-dashboard create token admin-user

# Pentru token cu expirare mai lungÄƒ
kubectl -n kubernetes-dashboard create token admin-user --duration=24h
```

#### 7. PVC nu se bind-uie

**CauzÄƒ:** StorageClass lipseÈ™te sau PV nu existÄƒ

**SoluÈ›ie:**
```bash
# Check PVC status
kubectl get pvc mysql-pvc
kubectl describe pvc mysql-pvc

# Check StorageClass
kubectl get storageclass

# DacÄƒ lipseÈ™te, instaleazÄƒ:
# Minikube: automat existÄƒ
# Kind: needs local-path-provisioner
```

### ğŸ“Š Debugging Commands

```bash
# Pod debugging
kubectl get pods -A                          # Toate pods
kubectl describe pod <pod-name>              # Detalii pod
kubectl logs <pod-name> -f                   # Follow logs
kubectl exec -it <pod-name> -- /bin/bash     # Shell Ã®n pod

# Service debugging
kubectl get svc -A                           # Toate services
kubectl get endpoints <service-name>         # Service endpoints
kubectl port-forward svc/<service> 8080:8000 # Test direct

# Ingress debugging
kubectl get ingress -A
kubectl describe ingress <ingress-name>

# Events (FOARTE UTIL!)
kubectl get events --sort-by='.lastTimestamp'
kubectl get events -n kubernetes-dashboard --sort-by='.lastTimestamp'

# Resource usage
kubectl top nodes
kubectl top pods

# Full cluster status
kubectl get all -A
```

### ğŸ” Dashboard-Specific Issues

**Dashboard nu se Ã®ncarcÄƒ:**
```bash
# Check pod status
kubectl get pods -n kubernetes-dashboard

# Check logs
kubectl logs -n kubernetes-dashboard deployment/kubernetes-dashboard

# Restart deployment
kubectl rollout restart deployment/kubernetes-dashboard -n kubernetes-dashboard

# Verify RBAC
kubectl get sa,clusterrole,clusterrolebinding | grep dashboard
```

**Token nu funcÈ›ioneazÄƒ:**
```bash
# VerificÄƒ admin-user existÄƒ
kubectl get sa admin-user -n kubernetes-dashboard

# RecreeazÄƒ dacÄƒ lipseÈ™te
kubectl apply -f templates/dashboard/admin-user.yaml

# GenereazÄƒ token nou
kubectl -n kubernetes-dashboard create token admin-user
```

---

## ğŸ‰ Concluzie

AceastÄƒ arhitecturÄƒ demonstreazÄƒ:

âœ… **Microservicii** independente, scalabile
âœ… **Containerizare** cu Docker
âœ… **Orchestrare** cu Kubernetes
âœ… **Service Discovery** automat
âœ… **Persistent Storage** pentru date
âœ… **JWT Authentication** securizat
âœ… **Role-Based Access Control** (RBAC)
âœ… **Ingress Routing** pentru external access
âœ… **Management UI** prin Kubernetes Dashboard
âœ… **Infrastructure as Code** cu Helm
âœ… **Cloud-Native** best practices

AplicaÈ›ia este **production-ready** cu cÃ¢teva Ã®mbunÄƒtÄƒÈ›iri:
- Secrets management cu Vault/Sealed Secrets
- TLS certificates cu cert-manager
- Monitoring cu Prometheus + Grafana
- Centralized logging cu ELK/Loki
- CI/CD pipeline cu GitLab/ArgoCD
- Horizontal Pod Autoscaling
- Resource quotas È™i limits

**Proiectul este complet funcÈ›ional È™i gata de demonstraÈ›ie!** ğŸš€

---

**Documentat cu â¤ï¸ pentru proiectul Cloud Computing 2026**

